const n=`---
title: 矩阵乘法还能更快吗？从 Strassen 到 Winograd 的优化之路
date: 2026-01-12
category: AI系统性能
tags:
  - 矩阵乘法
  - GEMM
  - Strassen算法
  - Winograd算法
  - 深度学习优化
excerpt: 回顾矩阵乘法优化史，揭示为什么“乘法更快了，整体却没更快”的现实。
---


## 从 Strassen 到 Winograd：当算法遇上内存墙
在现代 AI 训练与推理系统中，几乎所有性能讨论，最终都会被压缩到一个看似朴素、但极其残酷的问题上：
**在给定内存带宽的前提下，矩阵乘法还能榨出多少有效算力？**

从计算图的角度看，Transformer 中的 attention、MLP，乃至卷积在 lowering 之后的形态，本质上都可以还原为一系列 GEMM；而从系统的角度看，这些算子又被同一套硬件约束所裁决——Tensor Core 的峰值吞吐，和 HBM 的持续带宽。两者之间的权衡，决定了优化是否真的“落地”。

这也是为什么，在 Tensor Core 的算力增长速度长期快于内存子系统演进的现实下，任何关于矩阵乘法的优化，最终都绕不开 Roofline 模型。算法本身是否更“聪明”，已经不再是核心问题；真正重要的是：**它是否改变了算术强度，或者至少没有让访存变得更糟。**

正是在这个背景下，再回头看 Strassen 和 Winograd 为代表的矩阵乘法优化算法，才会发现它们经常被误解。

问题并不是“这些算法有没有价值”，而是：

> **既然我们知道矩阵乘法在数学上可以“少做乘法”，为什么在今天的 AI 系统中，它们却始终不是主流？**

## O(n³) 并不是现代系统真正关心的复杂度
在教科书里，朴素矩阵乘法的 (O(n^3)) 往往被当作需要被“突破”的对象；但在真实系统中，这个复杂度本身几乎没有直接意义。系统真正关心的，是在特定 shape、特定数据类型、特定硬件下，一个 GEMM 的 **算术强度（FLOPs 与内存访问量的比值）** 到底能不能把算力吃满。

在理想情况下，一个 GEMM 的算术强度大致可以写成：

$$
AI \\approx \\frac{2mnk}{(mk + kn + mn)\\cdot sizeof(dtype)}
$$

只有当 m,n,k 同阶且足够大时，算术强度才会随规模增长，计算才能稳定地落在 compute-bound 区域。这也是为什么，大规模训练时的 GEMM 看起来“什么都不用做就已经很快了”。

但一旦进入 Transformer 推理，尤其是 decode 阶段，情况立刻发生变化：
batch size 被压缩，m,n 变小，k 虽然仍然很大，但重用受限，流水线深度不足以隐藏访存延迟。此时，系统面对的已经不是“乘法算不算得完”，而是：

> **数据到底能不能及时喂进来。**

也正是在这种场景下，“减少乘法次数”才重新变得有吸引力。前提是，这种乘法次数的减少在不增加访存的前提下。Strassen 和 Winograd，这两种矩阵乘法优化算法，正是沿着这条思路给出的两种典型答案。

## Strassen：用代数重排减少乘法次数

### 核心原理

从纯算法角度看，Strassen 矩阵乘法算法的思想非常简洁：通过代数重排，把一次矩阵乘法拆解成更少的乘法和更多的加减。在 $2×2$ 的基本块上，它用 7 次乘法代替了 8 次；递归展开后，复杂度从 $(O(n^3))$ 降到了 $(O(n^{\\log_2 7}))$。

具体以 $2×2$ 矩阵乘法为例：

$$
C = A \\times B
$$

朴素算法需要 8 次乘法。而 Voker Strassen 的关键观察是：

**通过构造线性组合，可以用 7 次乘法 + 若干次加减，恢复完整结果。**

具体做法是将 $A$、$B$ 分块为子矩阵：

$$
\\begin{aligned}
A &= \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix} \\\\
B &= \\begin{bmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\end{bmatrix}
\\end{aligned}
$$

然后构造 7 个中间乘积：

$$
\\begin{aligned}
M_1 &= (A_{11}+A_{22})(B_{11}+B_{22}) \\\\
M_2 &= (A_{21}+A_{22})B_{11} \\\\
M_3 &= A_{11}(B_{12}-B_{22}) \\\\
M_4 &= A_{22}(B_{21}-B_{11}) \\\\
M_5 &= (A_{11}+A_{12})B_{22} \\\\
M_6 &= (A_{21}-A_{11})(B_{11}+B_{12}) \\\\
M_7 &= (A_{12}-A_{22})(B_{21}+B_{22})
\\end{aligned}
$$

借助这 7 个中间乘积，通过以下线性组合可精准恢复结果矩阵 $C = \\begin{bmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\end{bmatrix}$ 的各个子块：

$$
\\begin{aligned}
C_{11} &= M_1 + M_4 - M_5 + M_7 \\\\
C_{12} &= M_3 + M_5 \\\\
C_{21} &= M_2 + M_4 \\\\
C_{22} &= M_1 - M_2 + M_3 + M_6
\\end{aligned}
$$ 

这组线性组合的关键前提是代数等价性严格成立，并非近似替代，这也确保了乘法次数从 8 次减少到 7 次的有效性，是整个算法优化的核心基础。

递归应用后，时间复杂度下降到：

$$
O(n^{\\log_2 7}) \\approx O(n^{2.807})
$$

### 系统视角下的 Strassen：问题出在哪里？

那么，~~古尔丹（划掉）~~，代价是什么呢？

首先要明确的是，Strassen 算法减少乘法次数的代价并非微小的边角问题，而是贯穿整个计算流程的核心矛盾：乘法次数减少的同时，加减法的数量会显著增加，更关键的是整个计算过程需要引入大量的中间矩阵来存储这些线性组合的结果，而这些中间矩阵的访问模式又完全打破了朴素矩阵乘法中连续、规则的内存访问逻辑。

Roofline 模型的核心是通过算术强度来判断程序的性能瓶颈，要么受限于 compute-bound，要么受限于 memory-bound。

Strassen 算法虽然减少了 FLOPs 的总量，但内存流量并没有随之等比例下降——大量中间矩阵的读写导致内存访问量的下降幅度远小于乘法次数的下降幅度，这直接拉低了整个程序的算术强度。对于 GPU 这种高并行、高计算密度的硬件来说，**算术强度的降低会让程序更早地触达内存带宽的上限**，也就是所谓的 memory roof，就算 GPU 的计算核心还有大量空闲，整体性能也无法再提升。

而且，GPU 的高效运行依赖于规整的并行任务划分和连续的内存访问，而Strassen 算法的递归拆解会把原本大规模、规整的矩阵乘法拆分成大量小规模、不规则的子任务，这些子任务的调度需要额外的开销，同时频繁的递归调用也会占用更多的硬件资源，进一步抵消了乘法次数减少带来的收益。

AI训练和推理中的 GEMM 形状往往高度不规则，不像理论分析中那样可以假设为2的幂次规模，这会让 Strassen 算法的递归拆解变得更加复杂，额外开销进一步增加；同时AI场景中广泛使用 FP16 或 BF16 等低精度数据类型，Strassen 算法中大量的加减运算会放大数值误差，这种误差在深度神经网络的迭代训练中可能会累积，影响模型的收敛性和最终精度；再加上GPU上递归调度的高成本，最终结果是：**Strassen 在 AI 框架中几乎没有稳定的“甜点区间”。**

包括 Coppersmith–Winograd 系列，以及 AlphaTensor 发现的低阶乘法方案，本质上也都停留在：**固定 shape 的代数优化 ≈ 手写 micro-kernel** 而非可泛化的系统解法。

## Winograd：在变换域中压缩乘法

### 核心原理

对于传统的二维卷积，若将其展开为矩阵乘法 $Y = W \\times X$，其计算量是乘法次数的线性累加。然而，基于 Toom-Cook 算法思想，Shmuel Winograd 证明了一条关键定理：在有限域内，通过多项式插值，可以将乘法次数以指数级降低，代价是增加加法次数。对于深度学习中最常见的 $F(m, r)$ 形式（即 $r \\times r$ 的滤波器生成 $m \\times m$ 的输出块），我们以 $F(2,3)$ 为例进行严格推导。

首先，将一维卷积视为多项式乘法。设输入数据 $d$ 和滤波器 $g$ 的多项式表示分别为 

$$
d(x) = d_0 + d_1x + d_2x^2 + d_3x^3
$$ 

和 

$$
g(x) = g_0 + g_1x + g_2x^2
$$

我们需要计算乘积 

$$
s(x) = g(x)d(x)
$$

并最终保留其结果的前两项系数（即输出 $2 \\times 2$ 的块对应的一维情况）。

根据中国余数定理，如果我们选择模数 $M(x) = x(x-1)(x+1)(x^2)$，由于 $s(x)$ 的次数最高为 5，而我们需要的是 $s(x) \\mod x^2$，即 $s_0 + s_1x$。

为了计算这个结果，我们在特定的点集 $S = \\{0, 1, -1, \\infty\\}$ 上对多项式进行求值（点值表示），这对应于线性变换（即最高次项系数相乘）：

$$
\\begin{aligned}
m_0 &= g(0)d(0) = g_0 \\cdot d_0 \\\\
m_1 &= g(1)d(1) = (g_0+g_1+g_2) \\cdot (d_0+d_1+d_2+d_3) \\\\
m_2 &= g(-1)d(-1) = (g_0-g_1+g_2) \\cdot (d_0-d_1+d_2-d_3) \\\\
m_3 &= g(\\infty)d(\\infty) = g_2 \\cdot d_3 \\\\
\\end{aligned}
$$

此时，原本需要 $2 \\times 3 = 6$ 次乘法的一维卷积，被转化为 4 次独立的点值乘法。随后，通过逆变换（拉格朗日插值）从这四个点恢复出 $s_0, s_1$：

$$
\\begin{aligned}
s_0 &= m_0 \\\\
s_1 &= m_0 + m_1 + m_2 + m_3 \\\\
\\end{aligned}
$$


（具体系数需根据插值公式调整，此处展示逻辑流）
这一过程在二维上是可分离的。对于 $2 \\times 2$ 输出和 $3 \\times 3$ 滤波器的二维卷积 $F(2,3)$，我们只需先对行进行上述变换，再对列进行变换。最终，这一过程等价于矩阵形式 

$$
Y = A^T (G \\odot D) A
$$

其中 $G$ 和 $D$ 是变换后的滤波器和输入块，$\\odot$ 是 Hadamard 积（元素级乘法），$A$ 是变换矩阵。

### 系统视角下的 Winograd：为什么它“只对一半场景成立”？

在朴素方法中，计算一个 $2 \\times 2$ 的输出块需要 $4 \\times 9 = 36$ 次乘法。而在 Winograd 域内，$A$ 是 $4 \\times 4$ 矩阵，$G \\odot D$ 仅需 $4 \\times 4 = 16$ 次元素级乘法。乘法减少了约 55%。这对系统架构师而言意味着一个极具诱惑力的权衡：现代硬件（尤其是早期的 GPU 和移动端 NPU）中，乘法器（MAC）是占据面积和功耗最大的单元，而加法器相对廉价。Winograd 实际上是用廉价的加法和数据搬运，置换了昂贵的乘法操作。

正是这种算术强度的重构，使得 cuDNN、oneDNN 以及 NCNN 等底层库在很长一段时间内将 Winograd 视为 $3 \\times 3$ 卷积（ResNet 等架构的基石）的标准配置。

在 stride=1、padding=1 的标准场景下，这种变换将计算密度推向了极致。对于 Apple Neural Engine 或 Qualcomm Hexagon 这种受限于功耗和面积的移动端推理引擎，减少一半的 MAC 意味着在同等发热下 1.5 倍至 2 倍的吞吐量提升，这是单纯提升频率无法企及的。

然而，时代变了。

**变换矩阵 $A$ 的引入意味着输入数据不能直接进入计算单元，而是必须先经过“数据变换”阶段**。这一阶段需要额外的缓冲区来存储变换后的 Tile，且访问模式往往是不连续的。这导致了数据搬运量的激增——大约增加了 2-3 倍的数据读取和写入。在 2026 年的云端大模型时代，计算单元的算力往往溢出，而 HBM（高带宽内存）的带宽成为绝对瓶颈。对于 ViT 或混合架构中的 Linear 层，以及非方阵或大 stride 的卷积，Winograd 所需的额外数据搬运成本远超其节省的乘法收益，导致其在云端的使用率大幅下降。

还有一点，变换矩阵 $A$ 中通常包含分数系数（如 $1/2, -1/2, 1/4$），这在浮点数（FP32/FP16）运算中尚可接受，但在常用的 INT8 低比特量化场景下，会引入显著的舍入误差，导致推理精度下降。这迫使在 INT8 推理中不得不放弃 Winograd，转而回退到直接卷积或其他优化算法。

于是可以得到一个非常系统的结论：

> **Winograd 算法的成功并不是算法本身“更聪明”，而是它恰好匹配了某一代硬件的性能结构。**

在乘法器昂贵、带宽相对充裕的时代，Winograd 算法通过变换域压缩乘法次数，极大提升了算术强度；但在高带宽、低延迟的现代系统中，其增加的数据搬运和复杂的内存访问模式反而降低了整体性能。

## 优化了矩阵乘法，为什么 AI 还是卡？

Strassen 算法和 Winograd 算法代表了两种优化范式：递归降维和变换域压缩。它们确实在特定场景下减少了乘法次数，甚至在某些库中落地。但为什么在 2026 年的 AI 系统中，它们没能成为主流？

答案是：**这些优化主要针对“计算墙”，但 AI 的真实瓶颈早已转移到“内存墙”**。减少乘法的同时，往往增加加法和数据搬运——而在现代 GPU 上，带宽远比算力珍贵。结果：把乘法做得更快了，但数据喂不进来，整体性能反而没变。

这也引出系列的下一个问题：即使用最优算法实现了矩阵乘法，在 Transformer 的实际执行中，为什么性能还是被内存带宽卡死？下一期，我们通过 Roofline 模型剖析 Transformer 的算术强度，揭示 decode 阶段的“饿死在门口”真相。`;export{n as default};
